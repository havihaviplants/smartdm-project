import os
import json
import gspread
from typing import Dict
from oauth2client.service_account import ServiceAccountCredentials
import requests
from bs4 import BeautifulSoup
from pathlib import Path


# -------------------- ë§¤ë‰´ì–¼ ë¶ˆëŸ¬ì˜¤ê¸° (JSON ê¸°ë°˜) --------------------
def get_manual_text() -> str:
    manual_path = Path("utils/manual.json")
    if not manual_path.exists():
        return "ìƒë‹´ ë§¤ë‰´ì–¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    try:
        with manual_path.open(encoding="utf-8") as f:
            manual_data = json.load(f)

        if isinstance(manual_data, list):
            return "\n\n".join(f"Q: {item.get('Q', '')}\nA: {item.get('A', '')}" for item in manual_data)
        elif isinstance(manual_data, dict):
            return "\n\n".join(f"Q: {k}\nA: {v}" for k, v in manual_data.items())
        else:
            return "ìƒë‹´ ë§¤ë‰´ì–¼ í¬ë§· ì˜¤ë¥˜: list ë˜ëŠ” dict í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤."

    except Exception as e:
        return f"ìƒë‹´ ë§¤ë‰´ì–¼ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}"

# -------------------- êµ¬ê¸€ ì‹œíŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° --------------------
def get_google_client():
    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    creds = ServiceAccountCredentials.from_json_keyfile_name(
        os.getenv("GOOGLE_SERVICE_ACCOUNT_JSON"), scope
    )
    return gspread.authorize(creds)

def get_sheet_info() -> str:
    """
    êµ¬ê¸€ ì‹œíŠ¸ ë°ì´í„°ë¥¼ ìš”ì•½ëœ í…ìŠ¤íŠ¸ë¡œ íŒŒì‹±
    """
    try:
        sheet_id = os.getenv("GOOGLE_SHEET_ID")
        client = get_google_client()
        sheet = client.open_by_key(sheet_id).sheet1
        data = sheet.get_all_values()

        if not data or len(data) < 2:
            return "ì‹œíŠ¸ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        headers = [h.strip().lower() for h in data[0]]
        rows = data[1:]

        text_rows = []
        for row in rows:
            if all(not cell.strip() for cell in row):
                continue  # ì™„ì „íˆ ë¹ˆ í–‰ì€ ë¬´ì‹œ

            entry = []
            for h, v in zip(headers, row):
                v = v.strip()
                if not v or h in ['ë¹„ê³ ', 'ì°¸ê³ ']:  # ë¬´ì˜ë¯¸í•œ ì—´ ì œê±°
                    continue
                entry.append(f"{h}: {v}")

            if entry:
                text_rows.append(", ".join(entry))

        return "\n".join(text_rows)

    except Exception as e:
        return f"ì‹œíŠ¸ íŒŒì‹± ì‹¤íŒ¨: {e}"

# -------------------- êµ¬ê¸€ ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸° --------------------
def parse_doc() -> str:
    """
    êµ¬ê¸€ ë¬¸ì„œë¥¼ íŒŒì‹±í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    """
    try:
        doc_url = os.getenv("GOOGLE_DOC_URL")
        response = requests.get(doc_url)

        if not response.ok:
            return f"ë¬¸ì„œ ì ‘ê·¼ ì‹¤íŒ¨: {response.status_code}"

        soup = BeautifulSoup(response.content, "html.parser")
        body = soup.find('body')
        if not body:
            return "ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: body íƒœê·¸ ì—†ìŒ"

        lines = []
        for tag in body.find_all(['h1', 'h2', 'h3', 'p', 'li']):
            text = tag.get_text().strip()

            # í•„í„°ë§ ê¸°ì¤€
            if not text:
                continue
            if any(text.lower().startswith(prefix) for prefix in ["ì°¸ê³ ", "ë¹„ê³ ", "ì¶”ê°€"]):
                continue

            if tag.name in ['h1', 'h2', 'h3']:
                lines.append(f"\nğŸ“Œ {text}\n")
            elif tag.name == 'li':
                if len(text) < 3:
                    continue
                lines.append(f"- {text}")
            elif tag.name == 'p':
                if len(text.split()) < 3:
                    continue
                lines.append(text)

        unique_lines = list(dict.fromkeys(lines))  # ìˆœì„œ ë³´ì¡´ + ì¤‘ë³µ ì œê±°
        return "\n".join(unique_lines).strip()

    except Exception as e:
        return f"ë¬¸ì„œ íŒŒì‹± ì‹¤íŒ¨: {e}"
